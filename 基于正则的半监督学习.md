### 基于一致性正则的半监督学习
##### 导读

半监督学习（semi-supervised learning，SSL）中最常见的三个概念：一致性正则（Consistency Regularization）、熵最小化（Entropy Minimization）以及伪标签（Pseudo Labeling），今天小编就主要讲一讲最近的一致性正在半监督上的使用以及相关的论文：

- 本文要提到的半监督论文
    - Π-Model
    - Temporal Ensembling
    - Mean Teacher
    - R-Drop

##### 概念
半监督学习是一种解决在机器学习或者深度学习中数据缺少的常用的方法，旨在使用少量的带有标注的数据和大量未标注的数据上进行有效的学习。SSL的主要目标就是使用无标签的数据获得接近甚至优于强监督学习的评价指标。

半监督学习的常见策略是在经典的训练方法中加入额外的损失函数常见的也就是一致性正则损失、熵最小化以及伪标签。而正则化损失常见的代价函数，比如MSE、KL散度损失等常见的距离度量函数都已作为具体的代价损失函数，随着Consistency Regularization的不断发展，也曾一度成为半监督深度学习史上的明星SOTA。

一致性正则损失（Consistency Regularization Loss）的主要思想是对于一个输入即使受到了微小的扰动，其输出结果也应该是相同的。从而确保 high-level semantic information 也能够被学习到，进而确保学习的鲁棒性和高判别性$^{5}$。譬如某人带上了帽子和其没有戴帽子的识别结果应该是一致的。一致性正则的做法虽然简单但是产生的结果确实十分有效的。

#### 论文
- Π-Model

<img src=images/WX20211027-160903@2x.png width=80% a'lign><br>
Π-Model是`Temporal Ensembling for Semi-Supervised Learning`这篇文章提出方法的两种之一，可以说是一致性正则方法中比较简单的方法了，通过对输入的图片进行增广以及Dropout进行扰动，得到通过两次的forward得到预测结果，Π-Model使用MSE Loss作为一致性正则的损失函数，来约束两次模型的预测结果，希望两次结果尽可能的一致，提高模型对于扰动的鲁棒性。
模型图片中$y_{i}$以及虚线则表示输入图像的label，虚线表示label的状态，$x_{i}$表示图像输入，$z_{i}$则表示模型的预测结果。
监督学习部分则是通过和常见的分类模型一致的交叉熵损失函数，$w(t)$则是无监督损失权重上升函数，来控制无监督损失的权重。

Π-Model的最终loss则是有监督的CE loss和无监督的一致性loss组成，通过$w(t)$来控制权重的系数：
$$loss=CE(z,y)+w(t)MSE(z,\widetilde{z})$$

- Temporal Ensembling
  
<img src=images/WX20211027-161029@2x.png align=center weight=90% /><br>
Temporal Ensembling则是对模型的训练过程进行了改进，Π-Model需要输入的图像两次经过同一个模型，Temporal Ensembling方法通过使用以前所有的epoch中的预测结果作为本epoch的$\widetilde{z}_{i}$，有没有看到和EMA的思想很像，这里面隐式的包含了EMA的思想。
WHAT？什么是EMA？
EMA（Exponential Moving Average）全称指数加权平均，我们知道`加权平均数`是这样的
$$V_{n}=\frac{\theta_{1}+\theta_{2}+\theta_{3}+\theta_{4}+\cdot\cdot\cdot+\theta_{n}}{n}$$
`指数加权平均`则是一种近似的求平均值的算法
$$V_{n}=\beta \theta_{n} + (1-\beta)V_{n-1}$$
表示n时刻的指数加权平均数的计算方法，其中$\theta_{n}$表示当前时刻的数值，$V_{n-1}$表示n-1时刻的指数加权平均。

在Temporal Ensembling模型中$\widetilde{z}_{i}$表示的上一个epoch的所有预测结果的加权指数平均，通过EMA的方式相对于Π-Model方法减少近一半的训练时间，并且在效果上还有了小幅的提升。

下面的两张表展示了Π-Model和Temporal Ensembling在**CIFAR-10**和**SVHN**上性能
<img src=images/WX20211027-174239@2x.png align=center weight=90% /><br>
<img src=images/WX20211027-174459@2x.png align=center weight=90% /><br>
Temporal Ensembling在不使用全量数据的时候优于Π-Model和强监督模型，在使用全量数据时Π-Model优于监督方法和Temporal Ensembling，相对于有监督的方法在CIFAR-10和SVHN上分别获得了将近4.9%和3.4%的性能提升。

另外，作者还通过打乱训练标签的方法对有监督的方法和Temporal Ensembling的方法进行了比较
<img src=images/WX20211028-155355@2x.png align=center weight=90% /><br>
实验表明，有监督的方法随着打乱的标签的数量的增加，准确率急剧的下降，但是Temporal Ensembling方法，只是随着错误标签的增多只是轻微的下降，由此可见，基于EMA的方法增强了模型的鲁棒性。

但是，Π-Model和Temporal Ensembling的缺点也很明显，Temporal Ensembling只利用了Epoch中的预测结果相对于每个Batch有一定的滞后性，另外如果对于大规模的数据集他需要维护一个巨大数据表和类型信息。

- Mean Teacher

<img src=images/WX20211027-161101@2x.png align=center weight=90% /><br>
Mean Teacher的核心思想是student模型既充当学生又充当老师，不同于Temporal Ensembling中对于以前的epoch的预测结果进行EMA，Mean Teacher则是使用学生的参数通过EMA方法来更新的老师模型，通过在线的预测和学生输入相同的图像来得到预测的结果进行正则化约束。Temporal Ensembling模型通过每个epoch的预测结果作为教师网络的输出$\widetilde{z}_{i}$,预测结果有一定的滞后性，Mean Tearcher则是通过对每个Batch的Student的权重通过EMA的方式来进行产生新的$\widetilde{z}_{i}$。

如上图所示，上图通过EMA使得$Student \rightarrow Tearcher$来更新实时教师网络的权重。Π-Model和Temporal Ensembling模型都是隐式的包含了Student和Teacher模型，而Mean Teacher则是显示的包含了两者，通过学生模型的输出结果和标签进行CEloss，教师网络则是和学生网络进行了MSE Loss。Mean Teacher巧妙通过数据上的EMA方式转换到了模型权重上，一定程度上解决了上面两种方法的劣势。Mean Teacher的权重更新形式：

$$\theta^{\prime}_{t}=\alpha\theta^{\prime}_{t-1}+(1-\alpha)\theta_{t}$$
其中，$\theta_{t}$表示t时刻的学生权重，$\theta^{\prime}_{t-1}$表示t-1时刻的通过EMA求的老师网络权重

作者在SVHN数据集上的实验也表明，使用EMA的策略的模型的loss曲线更加的平滑，在Mean Tearcher的教师模型往往优于学生模型。

<img src=images/WX20211029-163345@2x.png align=center weight=90% /><br>

- R-Drop
<img src=images/WX20211027-161240@2x.png align=center weight=90% /><br>
R-Drop本质上并不是属于半监督领域，但是其使用Dropout来正则化约束模型，从而可以使模型的鲁棒性，稳定的提升模型的性能。

R-Drop不同于其他方法使用增广的数据通过统一模型而是利用Dropout的特性通过使用同一组数据两次经过同一个模型来得到预测结果来得到一致的输出结果。

R-Drop的损失函数也是由两部分组成CE Loss和KL 散度损失：
$$\mathcal{L}^{i}=\mathcal{L}^{i}_{NLL}+\alpha\cdot\mathcal{L}^{i}_{KL}$$

$\alpha$取0.3时在transformer取得了最好的实验效果。
<div align=center><img src=images/WX20211029-170116@2x.png height=200px /><br></div>

R-Drop在自然语言和图像分类上通过微调都取得了SOTA的结果，进一步提升了原文的实验效果，无痛提点。
在语言翻译上：
<img src=images/WX20211029-165939@2x.png align=center weight=90% /><br>
在图像分类上：
<div align=center><img src=images/WX20211029-170431@2x.png  height=200px /><br></div>

#### 总结


#### 引用
- [Temporal Ensembling for Semi-Supervised Learning](https://arxiv.org/abs/1610.02242)
- [Mean teachers are better role models:Weight-averaged consistency targets improvesemi-supervised deep learning results](https://arxiv.org/abs/1703.01780)
- [R-Drop: Regularized Dropout for Neural Networks](https://arxiv.org/abs/2106.14448)
- [Realistic Evaluation of Deep Semi-Supervised Learning Algorithms](https://arxiv.org/abs/1804.09170)
- [浅析 Semi-Supervised Learning 中的 consistency 问题](https://blog.csdn.net/JYZhang_CVML/article/details/106817709)